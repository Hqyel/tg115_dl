# -*- coding: utf-8 -*-
"""çˆ¬è™«æ¨¡å—"""

import re
import signal
import time
from typing import Optional
from urllib.parse import unquote

import requests
from bs4 import BeautifulSoup

from src.channels.config import CHANNELS, HEADERS, REQUEST_DELAY, PAN_115_PATTERN, PAN_115_PATTERN_ALT, is_valid_115_url
from src.models.resource import Resource, CrawlState
from src.core.database import Database, StateManager


class ChannelCrawler:
  """Telegram é¢‘é“çˆ¬è™«"""

  def __init__(self, channel_id: str):
    if channel_id not in CHANNELS:
      raise ValueError(f"æœªçŸ¥é¢‘é“: {channel_id}")

    self.channel_id = channel_id
    self.channel_config = CHANNELS[channel_id]
    self.channel_url = self.channel_config["url"]
    self.parse_mode = self.channel_config["parse_mode"]

    self.session = requests.Session()
    self.session.headers.update(HEADERS)
    self._interrupted = False

  def setup_signal_handler(self, state_manager: StateManager, state: CrawlState):
    """è®¾ç½®ä¸­æ–­ä¿¡å·å¤„ç†"""
    def handler(signum, frame):
      print("\n\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
      state_manager.save(state)
      print(f"è¿›åº¦å·²ä¿å­˜ã€‚ä½¿ç”¨ --resume ç»§ç»­çˆ¬å–ã€‚")
      self._interrupted = True

    signal.signal(signal.SIGINT, handler)
    signal.signal(signal.SIGTERM, handler)

  def crawl_all(self, db: Database, state_manager: StateManager,
          resume_state: Optional[CrawlState] = None) -> int:
    """å…¨é‡çˆ¬å–"""
    if resume_state:
      state = resume_state
      print(f"ä»æ–­ç‚¹æ¢å¤ï¼Œå·²çˆ¬å–: {state.total_crawled}")
    else:
      state = CrawlState(channel_id=self.channel_id, mode="all")

    self.setup_signal_handler(state_manager, state)

    print(f"å¼€å§‹çˆ¬å–é¢‘é“: {self.channel_config['name']}")
    print(f"æ¨¡å¼: å…¨é‡çˆ¬å– (Ctrl+C å¯ä¸­æ–­ä¿å­˜)")
    print("-" * 50)

    current_before_id = state.last_before_id
    saved_count = state.total_crawled

    while not self._interrupted:
      url = self.channel_url
      if current_before_id:
        url = f"{self.channel_url}?before={current_before_id}"

      print(f"æ­£åœ¨è¯·æ±‚: {url}")

      try:
        response = self.session.get(url, timeout=30)
        response.raise_for_status()
      except requests.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}ï¼Œç­‰å¾… 10 ç§’é‡è¯•...")
        time.sleep(10)
        continue

      soup = BeautifulSoup(response.text, "lxml")
      messages = self._parse_messages(soup)

      if not messages:
        print("æ²¡æœ‰æ›´å¤šæ¶ˆæ¯äº†ï¼Œçˆ¬å–å®Œæˆï¼")
        state_manager.clear()
        break

      new_count = 0
      for msg in messages:
        if self._interrupted:
          break
        if not db.exists(self.channel_id, msg.message_id):
          db.save_resource(self.channel_id, msg)
          new_count += 1
          saved_count += 1

      print(f"æœ¬é¡µ: {len(messages)} æ¡ï¼Œæ–°å¢: {new_count} æ¡ï¼Œç´¯è®¡: {saved_count} æ¡")

      current_before_id = min(msg.message_id for msg in messages)
      state.last_before_id = current_before_id
      state.total_crawled = saved_count

      if saved_count % 100 == 0:
        state_manager.save(state)

      time.sleep(REQUEST_DELAY)

    print("-" * 50)
    print(f"çˆ¬å–å®Œæˆï¼Œå…±ä¿å­˜ {saved_count} æ¡èµ„æº")
    return saved_count

  def crawl_incremental(self, db: Database) -> int:
    """å¢é‡çˆ¬å–"""
    latest_id = db.get_latest_message_id(self.channel_id)

    print(f"å¼€å§‹å¢é‡çˆ¬å–: {self.channel_config['name']}")
    print(f"æ•°æ®åº“æœ€æ–°æ¶ˆæ¯ ID: {latest_id}")
    print("-" * 50)

    if latest_id == 0:
      print("æ•°æ®åº“ä¸ºç©ºï¼Œè¯·å…ˆä½¿ç”¨ --all è¿›è¡Œåˆå§‹çˆ¬å–")
      return 0

    new_count = 0
    current_before_id = None
    consecutive_exists = 0

    while consecutive_exists < 20:
      url = self.channel_url
      if current_before_id:
        url = f"{self.channel_url}?before={current_before_id}"

      print(f"æ­£åœ¨è¯·æ±‚: {url}")

      try:
        response = self.session.get(url, timeout=30)
        response.raise_for_status()
      except requests.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        break

      soup = BeautifulSoup(response.text, "lxml")
      messages = self._parse_messages(soup)

      if not messages:
        break

      page_new = 0
      for msg in messages:
        if db.exists(self.channel_id, msg.message_id):
          consecutive_exists += 1
        else:
          consecutive_exists = 0
          db.save_resource(self.channel_id, msg)
          page_new += 1
          new_count += 1

      print(f"æœ¬é¡µ: {len(messages)} æ¡ï¼Œæ–°å¢: {page_new} æ¡")

      current_before_id = min(msg.message_id for msg in messages)
      time.sleep(REQUEST_DELAY)

    print("-" * 50)
    print(f"å¢é‡çˆ¬å–å®Œæˆï¼Œæ–°å¢ {new_count} æ¡èµ„æº")
    return new_count

  def crawl_with_limit(self, limit: int, db: Database) -> int:
    """é™é‡çˆ¬å–"""
    print(f"å¼€å§‹çˆ¬å–: {self.channel_config['name']}")
    print(f"ç›®æ ‡æ•°é‡: {limit}")
    print("-" * 50)

    current_before_id = None
    saved_count = 0

    while saved_count < limit:
      url = self.channel_url
      if current_before_id:
        url = f"{self.channel_url}?before={current_before_id}"

      print(f"æ­£åœ¨è¯·æ±‚: {url}")

      try:
        response = self.session.get(url, timeout=30)
        response.raise_for_status()
      except requests.RequestException as e:
        print(f"è¯·æ±‚å¤±è´¥: {e}")
        break

      soup = BeautifulSoup(response.text, "lxml")
      messages = self._parse_messages(soup)

      if not messages:
        print("æ²¡æœ‰æ›´å¤šæ¶ˆæ¯äº†")
        break

      for msg in messages:
        if saved_count >= limit:
          break
        if not db.exists(self.channel_id, msg.message_id):
          db.save_resource(self.channel_id, msg)
          saved_count += 1

      print(f"æœ¬é¡µ: {len(messages)} æ¡ï¼Œç´¯è®¡: {saved_count} æ¡")

      current_before_id = min(msg.message_id for msg in messages)
      time.sleep(REQUEST_DELAY)

    print("-" * 50)
    print(f"çˆ¬å–å®Œæˆï¼Œå…±ä¿å­˜ {saved_count} æ¡èµ„æº")
    return saved_count

  def _parse_messages(self, soup: BeautifulSoup) -> list[Resource]:
    """è§£æé¡µé¢æ¶ˆæ¯"""
    resources = []
    message_divs = soup.find_all("div", class_="tgme_widget_message_wrap")

    for div in message_divs:
      try:
        resource = self._parse_single_message(div)
        if resource:
          resources.append(resource)
      except Exception as e:
        print(f"è§£ææ¶ˆæ¯å¤±è´¥: {e}")
        continue

    return resources

  def _parse_single_message(self, div) -> Optional[Resource]:
    """è§£æå•æ¡æ¶ˆæ¯"""
    message_elem = div.find("div", class_="tgme_widget_message")
    if not message_elem:
      return None

    data_post = message_elem.get("data-post", "")
    if "/" not in data_post:
      return None

    message_id = int(data_post.split("/")[-1])

    # æå–æ ‡ç­¾
    tags = []
    title_parts = []
    text_div = div.find("div", class_="tgme_widget_message_text")

    if text_div:
      for link in text_div.find_all("a", href=True):
        href = link.get("href", "")
        if "?q=%23" in href:
          tag_text = link.get_text(strip=True)
          if tag_text.startswith("#"):
            tags.append(tag_text)
            title_parts.append(tag_text.lstrip("#"))

    # æ ¹æ®è§£ææ¨¡å¼å¤„ç†
    # è·å–æ¸…ç†åçš„åŸå§‹HTML
    raw_html = self._get_clean_html(div)

    if self.parse_mode == "telegraph":
      return self._parse_telegraph_mode(div, message_id, tags, title_parts, raw_html)
    elif self.parse_mode == "button":
      return self._parse_button_mode(div, message_id, tags, title_parts, text_div, raw_html)
    else:
      return self._parse_inline_mode(div, message_id, tags, title_parts, text_div, raw_html)

  def _get_clean_html(self, div) -> str:
    """è·å–æ¸…ç†åçš„æ¶ˆæ¯å¡ç‰‡HTML"""
    from copy import copy
    # å¤åˆ¶å…ƒç´ ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    div_copy = copy(div)
    # ç§»é™¤ä¸å¿…è¦çš„å…ƒç´ 
    for script in div_copy.find_all(['script', 'style']):
      script.decompose()
    return str(div_copy)

  def _parse_telegraph_mode(self, div, message_id: int, tags: list, title_parts: list, raw_html: str) -> Optional[Resource]:
    """è§£æ telegraph æ¨¡å¼ï¼ˆLsp115ï¼‰"""
    telegraph_url = ""
    for link in div.find_all("a", href=True):
      href = link.get("href", "")
      link_text = link.get_text(strip=True)
      if "telegra.ph" in href and ("æŸ¥çœ‹èµ„æº" in link_text or "ğŸ“" in link_text):
        telegraph_url = href
        break

    if not telegraph_url:
      return None

    title = self._extract_title_from_url(telegraph_url)
    if not title and title_parts:
      title = " ".join(title_parts)
    if not title:
      title = f"èµ„æº_{message_id}"

    return Resource(
      message_id=message_id,
      title=title,
      tags=",".join(tags),
      telegraph_url=telegraph_url,
      raw_html=raw_html
    )

  def _parse_inline_mode(self, div, message_id: int, tags: list, title_parts: list, text_div, raw_html: str) -> Optional[Resource]:
    """è§£æ inline æ¨¡å¼ï¼ˆvip115hotï¼‰"""
    pan_url = ""
    description = ""
    title = ""

    if text_div:
      text_content = text_div.get_text(separator="\n", strip=True)

      for link in text_div.find_all("a", href=True):
        href = link.get("href", "")
        if is_valid_115_url(href):
          pan_url = href
          break

      if not pan_url:
        match = PAN_115_PATTERN.search(text_content)
        if match:
          pan_url = match.group(0)
        else:
          match = PAN_115_PATTERN_ALT.search(text_content)
          if match:
            pan_url = match.group(0)

      lines = [l for l in text_content.split("\n") if l.strip()][:10]
      description = "\n".join(lines)

      # ä¼˜å…ˆä»æè¿°ä¸­æå–æ ‡é¢˜ï¼ˆ"åç§°ï¼š"åé¢çš„å†…å®¹ï¼‰
      for line in lines:
        if line.startswith("åç§°ï¼š") or line.startswith("åç§°:"):
          title = line.split("ï¼š", 1)[-1].split(":", 1)[-1].strip()
          break

    if not is_valid_115_url(pan_url):
      return None

    # å¦‚æœæ²¡æœ‰ä»æè¿°æå–åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨æ ‡ç­¾æˆ–é»˜è®¤å€¼
    if not title:
      if title_parts:
        title = " ".join(title_parts[:3])
      else:
        title = f"èµ„æº_{message_id}"

    return Resource(
      message_id=message_id,
      title=title,
      tags=",".join(tags),
      pan_url=pan_url,
      description=description,
      raw_html=raw_html
    )

  def _parse_button_mode(self, div, message_id: int, tags: list, title_parts: list, text_div, raw_html: str) -> Optional[Resource]:
    """è§£æ button æ¨¡å¼ï¼ˆQukanMovieï¼‰"""
    pan_url = ""
    description = ""
    title = ""

    for link in div.find_all("a", href=True):
      link_text = link.get_text(strip=True)
      href = link.get("href", "")
      if "ç‚¹å‡»è·³è½¬" in link_text:
        if is_valid_115_url(href):
          pan_url = href
          break

    if not is_valid_115_url(pan_url):
      return None

    if text_div:
      text_content = text_div.get_text(separator="\n", strip=True)
      lines = [l for l in text_content.split("\n") if l.strip()][:10]
      description = "\n".join(lines)

      # ä»æè¿°ä¸­æå–æ ‡é¢˜
      for line in lines:
        # è·³è¿‡çº¯è¡¨æƒ…ç¬¦å·çš„è¡Œ
        clean_line = re.sub(r'[\U0001F300-\U0001F9FF\u2600-\u26FF\u2700-\u27BF]', '', line).strip()
        if not clean_line:
          continue
        # æŸ¥æ‰¾åŒ…å«èµ„æºåç§°çš„è¡Œï¼ˆå¦‚ "ç”µè§†å‰§ï½œå‡¡äººä¿®ä»™ä¼ ..."ï¼‰
        if "ï½œ" in line or "|" in line:
          title = line.split("ï½œ", 1)[-1].split("|", 1)[-1].strip()
          break
        # æˆ–è€…å–ç¬¬ä¸€ä¸ªéè¡¨æƒ…ç¬¦å·çš„è¡Œ
        if not title and len(clean_line) > 2:
          title = clean_line
          break

    # å¦‚æœæ²¡æœ‰æå–åˆ°æ ‡é¢˜ï¼Œä½¿ç”¨æ ‡ç­¾æˆ–é»˜è®¤å€¼
    if not title:
      if title_parts:
        title = " ".join(title_parts[:3])
      else:
        title = f"èµ„æº_{message_id}"

    return Resource(
      message_id=message_id,
      title=title,
      tags=",".join(tags),
      pan_url=pan_url,
      description=description,
      raw_html=raw_html
    )

  def _extract_title_from_url(self, url: str) -> str:
    """ä» telegraph URL æå–æ ‡é¢˜"""
    try:
      path = url.split("telegra.ph/")[-1]
      title = unquote(path)
      title = re.sub(r'-\d{2}-\d{2}(-\d+)?$', '', title)
      return title
    except Exception:
      return ""
